# -*- coding: utf-8 -*-
"""Flipr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1829QeuaabgTHQJIgShhyZJJwnYklp35r
"""

pip install git+https://github.com/tensorflow/docs

import pandas as pd
from sklearn import preprocessing 
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling

df = pd.read_excel("Train_dataset.xlsx")
df.tail()

df.isna().sum()

df = df.dropna()  #dropping all rows with missing values. - data cleaning

df.tail()

output =np.array(df.loc[:,"Infect_Prob"])

for i in range(len(output)):
  output[i] = output[i]/100

output

df = df.drop('Infect_Prob', axis=1)
df = df.drop('people_ID',axis=1)

#df1 = pd.get_dummies(df['Region'],prefix='Region')
#df = pd.concat([df,df1],axis =1)
catg = ['Mode_transport', 'comorbidity', 'cardiological pressure','Pulmonary score'] #catagorical data
for i in range(4):
  df1 = pd.get_dummies(df[catg[i]],prefix=catg[i])
  df = pd.concat([df,df1],axis =1)
  df = df.drop(catg[i],axis=1)
df = df.drop(['Designation','Name','Region','Occupation'],axis = 1)  #redundant data- we remove this to make the model more effecient 
label_encoder = preprocessing.LabelEncoder()
df['Married'] = label_encoder.fit_transform(df['Married'])  #label encoding
df['Gender'] = label_encoder.fit_transform(df['Gender'])

stats = df.describe()
stats = stats.transpose()
stats

def build_model():   # we change and tune the hyper parameters of this model to give least loss
  model = keras.models.Sequential()
  model.add(layers.Dense(128,input_shape=(33,)))
  #model.add(layers.Dropout(0.1))
  model.add(layers.Dense(128,activation='relu'))
  model.add(layers.Dropout(0.2))
  model.add(layers.Dense(64,activation = 'relu'))
  model.add(layers.Dropout(0.2))
  model.add(layers.Dense(32,activation = 'relu'))
  model.add(layers.Dense(1, activation = 'sigmoid'))
  model.compile(optimizer='adam', loss= 'mse')
  return model

model = build_model()
model.summary()

#ex = df[:10]
#ex_r = model.predict(ex)
#ex_r

def norm(x):
  return ((x - stats['mean']) / stats['std'])
norm_data = norm(df)
norm_data.tail()

EPOCHS = 1000
history = model.fit(norm_data,output,epochs=EPOCHS, validation_split = 0.2, verbose=0,callbacks=[tfdocs.modeling.EpochDots()])
# we give a validation set to avoid over-fitting

"""Accuracy stays zero because we are trying to REGRESS the probability of infection and not classify people."""

#Data set cleaning for Test data
dft = pd.read_excel("Test_dataset.xlsx")
pid =np.array(dft.loc[:,"people_ID"])
pid =pd.DataFrame(pid)
dft = dft.drop('people_ID',axis=1)

catg = ['Mode_transport', 'comorbidity', 'cardiological pressure','Pulmonary score']
for i in range(4):
  df1 = pd.get_dummies(dft[catg[i]],prefix=catg[i])
  dft = pd.concat([dft,df1],axis =1)
  dft = dft.drop(catg[i],axis=1)
dft = dft.drop(['Region','Designation','Name','Occupation'],axis = 1)

label_encoder = preprocessing.LabelEncoder()
dft['Married'] = label_encoder.fit_transform(dft['Married'])
dft['Gender'] = label_encoder.fit_transform(dft['Gender'])

norm_test = norm(dft)
norm_test.tail()
# we use the statistics of the training data set for the test data set too.

predicted = model.predict(norm_test)

type(predicted)

print(predicted)
for i in range (len(predicted)):
  predicted[i] = predicted[i] * 100
predicted= pd.DataFrame(predicted)
predicted = pd.concat([pid,predicted],axis=1)
predicted.to_csv("results.csv")

